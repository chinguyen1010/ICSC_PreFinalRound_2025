# ICSC_PreFinalRound_2025
Solutions and report for the International Computer Science Competition (Pre-Final Round 2025)

[ðŸ“„ View the main report (PDF)](./ICSC_Pre_Final_Round_2025.pdf)  
[ðŸ“˜ View the submission paper (PDF)](./ISCS%20pre-final%20round%202025-%20Chi%20Nguyen.pdf)



<img width="594" height="557" alt="Screenshot 2025-11-02 at 9 58 34â€¯PM" src="https://github.com/user-attachments/assets/75de4c17-e976-47e1-899c-7b70add8f49e" />



<img width="665" height="350" alt="Screenshot 2025-11-02 at 9 59 06â€¯PM" src="https://github.com/user-attachments/assets/fe5571d2-8c5b-4caa-a001-43980515f8ef" />



<img width="554" height="718" alt="Screenshot 2025-11-02 at 9 59 35â€¯PM" src="https://github.com/user-attachments/assets/afe24db7-c37b-4265-897b-b31ef60adc12" />



<img width="565" height="415" alt="Screenshot 2025-11-02 at 10 00 11â€¯PM" src="https://github.com/user-attachments/assets/eb32b2a9-3449-4ac7-9180-7d6f75c92766" />












 Problem C.1: Zipfâ€™s Meaning-Frequency Law (8 Points) 
 This problem requires you to read the following recently published scientific article:

 A New Formulation of Zipfâ€™s Meaning-Frequency Law through Contextual Diversity by Ryo Nagata and Kumiko Tanaka-Ishii (2025)
 Link: https://aclanthology.org/2025.acl-long.744.pdf

Answer the following questions related to this article:
(a) What are the limitations of dictionary-based studies on measuring Zipfâ€™s Meaning-Frequency Law?

(b) Explain the von Mises-Fisher distribution and how v = 1/Îº measures contextual diversity.

(c) Why do the authors use the von Mises-Fisher distribution instead of simpler measures like average pairwise cosine similarity between word vectors?

(d) How do autoregressive models compare to masked language models for observing the Meaning- Frequency law?

(e) How can the proposed method serve as a diagnostic tool for language models?

(f) What does the observation that meaning-frequency law breaks down for small models and out-of-domain data suggest?

(Bonus) What factors may lead more frequent words to have more meanings? What factors may lead to fewer meanings? Give examples of each.


Problem C.2: Self-Improvement Capabilities of LLMs (8 Points)
This problem requires you to read the following recently published scientific article:
Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models by Y. Song, H. Zhang, C. Eisenach, S. M. Kakade, D. Foster, and U. Ghai (2025).
Link: https://openreview.net/pdf?id=mtJSMcF3ek

Answer the following questions related to this article:

(a) Describe the term self-improvement using the authorâ€™s framework. What key assumption are the authors making that allows for self-improvement?

(b) What is the generation-verification gap (GV-Gap)? Why is it a better metric than measuring performance differences after model updates?

(c) What is greedy decoding and why is self-improvement with greedy decoding impossible?

(d) Explain why the relative GV-Gap scales monotonically with pre-training FLOPs for certain verification methods but not others.

(e) Why do most models fail to self-improve on Sudoku puzzles despite the exponential com- putational complexity separation between generation and verification?

(f) Propose a task domain where you would expect self-improvement to improve performance and explain why.


